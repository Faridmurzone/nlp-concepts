Natural Language Processing (NLP) es la rama del Machine Learning dedicada a procesar el lenguaje.
[Wikipedia lo define](https://es.wikipedia.org/wiki/Procesamiento_de_lenguajes_naturales
) como un campo de las ciencias de la computación, inteligencia artificial y lingüística que estudia las interacciones entre las computadoras y el lenguaje humano. El NLP se ocupa de la formulación e investigación de mecanismos eficaces computacionalmente para la comunicación entre personas y máquinas por medio del lenguaje natural, es decir, de las lenguas del mundo.

Cuando decimos interacciones entre las computadoras y el lenguaje humano se nos vienen rápidamente a la mente los asistentes como Google Assistant, Siri, Alexa o Cortana. Sí, todas ellas son formas de NLP. Pero el NLP está mucho más presente en nuestra vida cotidiana. En stackoverflow cuando hacemos una pregunta y nos dice si ya fue previamente realizada o nos recomienda tags. En los sistemas de recomendación de redes sociales y plataformas multimedia, donde cáda vez que nos recomiendan una serie o una publicación se analizan las descripciones, etiquetas, actores, géneros y gustos de otras personas con mis datos previos. Nos comunicamos permanentemente con las máquinas, a veces de forma consciente y a veces sin saberlo. Al poner like en una publicación, al escuchar la misma canción o ver la misma serie más de una vez, al elegir siempre películas del mísmo género y con etiquetas muy similares, al taguear un email como Spam o hasta ir a determinados lugares utilizando google maps.
Todas estas interacciones son estudiadas por varios campos del Machine Learning, pero también pueden ser interpretadas como interacciones de lenguaje, instrucciones y deseos manifestados por nosotros explícita o implicitamente.

El NLP se estudia desde la matemática y especialmente desde la estadística. Pero también entran en juego la lingüistica y otras disciplinas.
Es importante para comprender una comunicación estudiar la morfología de la misma, pero también su sintaxis, su semántica y su pragmática. Según la rama dentro del NLP puede ser conveniente estudiar la fonología humana o según el objetivo puede aportar al análisis del discurso.

Con la morfología estudiamos las palábras, con la sintáxis introducimos la relación entre las palabras y cómo se componen (por ejemplo donde está un sujeto y donde un predicado), en la semántica ya nos vamos al plano del significado y llegamos al plano donde el SOTA se está superando ahora. Finalmente, complementamos el análisis con la pragmática, el contexto en el que ese significado tiene sentido.

## Historia del NLP

El NLP surge aproximadamente en los 60's con los primeros análisis de texto mediante computadora. Un hito para el NLP y la lingüistica fue fue la publicación de *Computational Analysis of Present-Day American English* en 1967 de los autores Henry Kucera y W. Nelson Francis. En la publicación exponen cómo realizaron computacionalmente el primer corpus de NLP, *Brown Corpus of Standard American English*. Este corpus contenía aproximadamente un millón de palabras. 

Este trabajo fue el puntapié para que el NLP comenzara a poner foco en la extracción y análisis de elementos lexicales, en los métodos de estados finitos y en las primeras traducciones automáticas.

En los 70's se avanza en el análisis de texto y en la creación de técnicas lógicas y de razonamiento para extraer conocimiento de un texto.

En los 80's la estadística entra con fuerza al campo para analizar las probabilidades basadas en pesos de que una palabra se encuentre en determinado lugar de la oración. Con esto aparecen también los primeros algoritmos de aprendizaje automático (Machine Learning) supervisados.

Los 90's ya comienzan a anticipar una explosión en el campo. En primer lugar el crecimiento exponencial en el poder computacional y en la cantidad de datos almacenados a partir de la explosión de internet permiten extraer grandes cantidades de información de la red. Comienzan a desarrollarse algoritmos para reconocimiento de voz (Speech to text o speech recognition) y surgen algunas técnicas populares aún hoy como Named Entity Recognition (NER) o Part-of-speech tagging.

Los 2000 explotan al máximo lo logrado en la última década. Los buscadores web son los que se llevan los focos de las cámaras, Google se lleva todos los laureles. No sólo por el sistema de búsquedas sino también por la solidez de Google Translate. Microsoft hace lo suyo en aplicaciones del paquete office, particularmente en Word, reconociendo y recomendando con mucha precisión ya no sólo errores en la ortografía sino en la gramática. También crean CNTK y [MSRLM](https://www.microsoft.com/en-us/research/uploads/prod/2017/01/TR-2007-144.pdf), dos toolkits para NLP. El segundo ya incorpora 40 mil millones de palabras. Recordemos el salto desde 1967 de 1 millón a 2007 de 40mil millones.

La última década fue una verdadera revolución tecnológica encabezada por la Inteligencia Artificial y el análisis de datos. Para no ser menos el NLP ha tenido unos últimos 3 o 4 años en los que mes a mes se supera el State of The Art (SOTA)... y por mucho. Es habitual leer en las noticias información relativa a algún algoritmo nuevo que permite crear un texto, traducir en realtime lo que decimos o hasta programar un componente de software. 
En 2018 llegó una de las grandes novedades de la década: BERT. Representación de Codificador Bidireccional de Transformadores (BERT, por sus siglas en inglés) es una técnica basada en redes neuronales elaborada por Google. La gran novedad de BERT es la bidireccionalidad, es decir que juzga las palabras tanto a la izquierda como a la derecha de cada uno de los términos. El SOTA previo (como word2vec o Glove) generaban representaciones únicas para cada palabra, en cambio BERT otorga significado a partir del contexto. 

Por ejemplo: 
```python
Pedro tiene un perro
Pedro es un perro jugando al futbol
```
En la primera frase perro hace referencia al sustantivo animal, mientras que en la segunda hace referencia al adjetivo de calidad de Pedro jugando al futbol. Word2vec le otorgaría el mismo significado a Perro en ambos casos, BERT no. Al día de hoy BERT está disponible en más de 70 idiomas y es utilizado oficialmente por Google en varios de sus productos (inclusive los buscadores).

Además de BERT la década estuvo plagada de avances en NLP. En los últimos años en especial se han logrado grandes hitos casi mes a mes utilizando aprendizaje profundo. En pocos años hemos visto el éxito de las redes neuronales con LSTM y GRU, combinadas con mecanismos de atención y bidireccionalidad, combinadas en etapas jerárquicas, los modelos de atención como Self-attention, Bahdanau, Luong y Multi-Head. Y a partir de estos últimos especialmente la proliferación de Transformers, reformers, revnets. Todos estos conceptos y otros los expondré en profundidad en artículos en el futuro.

El año 2020 ha sido el año de los modelos generativos y a la cabeza de muchos ya habrá venido GPT-3. Generative Pre-trained Transformer 3 (GPT-3) es (ya no pero casi) la última novedad en NLP. Vino a superar el Turing Natural Language Generation (T-NLG) de Microsoft presentado apenas unos meses antes. GPT-3 entrena un corpus con unos de 175mil millones de parámetros en su versión completa, comparados con los 17mil millones de su versión previa GPT-2. GPT-3 ha sido capás de redactar artículos científicos enteros, noticias, programar scripts enteros en diferentes lenguajes y responder preguntas sobre sí mismo. En todos estos casos haciendo casi imperceptible la diferencia entre un artículo escrito por un humano y una computadora. Esto fue posible gracias a millones de elementos tomados de Comun Crawl, Wikipedia y Books. Todas estas fuentes abiertas permiten hoy a la comunidad científica seguir desarrollando modelos 
data-hungry. En los últimos meses han aparecido datasets nuevos que aportan más leña para el fuego como el de arXiv que contienen 1,7 millones de papers científicos. Los algoritmos se siguen mejorando. Algunos resultados preliminares como los [https://arxiv.org/abs/2009.07118](publicados por Timo Schick, Hinrich Schütze) permiten vislumbrar que con un 0,01% de los parámetros utilizados por GPT-3 se puede igualar y hasta superar su performance.

Estamos ante un horizonte realmente excitante. Su alcance aún no es claro, el NLP llegó para quedarse y para cambiar radicalmente el mundo tal cual lo conocemos.
